<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta property="og:url" content="https://www.liunaijie.top/coding/big_data/spark/spark%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/">
  <meta property="og:site_name" content="Jarvis`s library">
  <meta property="og:title" content="Spark-参数配置">
  <meta property="og:description" content="https://spark.apache.org/docs/latest/configuration.html">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="coding">
    <meta property="article:published_time" content="2022-04-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2022-04-01T00:00:00+00:00">
    <meta property="article:tag" content="Spark">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Spark-参数配置">
  <meta name="twitter:description" content="https://spark.apache.org/docs/latest/configuration.html">

  
  
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#181818">
  <title>
    
    Jarvis`s library - Spark-参数配置
    
  </title>
  
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  
  
  
  <link rel="stylesheet" href="/minima.54cfcb44e10b4015b41a13771763013b79bdba6a92e49ea4a77bb44db465e761.css" integrity="sha256-VM/LROELQBW0GhN3F2MBO3m9umqS5J6kp3u0TbRl52E=">
  
  
  
  <script defer type="text/javascript" src="/minima.b4da24217e147f536fc7dc225886a1ea20bedabe7aed49e546a5d97cc34e4555.js" integrity="sha256-tNokIX4Uf1Nvx9wiWIah6iC&#43;2r567UnlRqXZfMNORVU="></script>
  
  
  
</head>
<script>
  const theme_config = 'system'
  const theme_light = theme_config === 'system' ? 'light' : theme_config;
  let theme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : theme_light;
  console.debug(theme);

  try {
    localStorage.setItem('theme', theme);
    window.minima_theme = theme;
    document.querySelector('html').classList.add(theme);
  } catch (e) {
    console.error(e);
  }
</script>



<body>
  <header class="mt-3 mb-8">
  <div class="container mx-auto">
    <nav class="flex justify-between items-center">
      <div class="flex items-center">
        
        <div id="theme-switch" class="text-2xl cursor-pointer"></div>
      </div>
      <ul class="flex items-center text-base font-semibold
        whitespace-nowrap overflow-x-auto overflow-y-hidden">
        
        <li class="ml-2 mr-2">
          
          <a href='/'>首页</a>
          
        </li>
        
        <li class="ml-2 mr-2">
          
          <a href="/tags">标签</a>
          
        </li>
        
        <li class="ml-2 mr-2">
          
          <a href="/search">搜索</a>
          
        </li>
        
        <li class="ml-2 mr-2">
          
          <a href="/about">关于</a>
          
        </li>
        
      </ul>
      <ul class="flex item-center text-sm font-semibold">
        
        <li class="ml-2"><a href="https://www.liunaijie.top/"></a></li>
        
      </ul>
    </nav>
  </div>
</header>

  
<div class="container mx-auto">
  <h1 class="text-4xl font-extrabold mt-6 mb-6">Spark-参数配置</h1>
  <div class="mb-3 text-sm flex justify-between ">
    <div>
      
      发布于 &mdash; 2022 年 04 月 01 日
      
      
    </div>
    
    <div>
      
      
      <a class="ml-1" href="/tags/Spark">#Spark</a>
      
    </div>
    
  </div>
  <main class="mb-8">
    <p></p>
    <article class="md">
      <p><a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a></p>
<h1 id="查看应用的配置">查看应用的配置</h1>
<p>打开Spark UI, 点击<code>Environment</code> tab, 就可以看到设置的参数</p>
<h1 id="设置">设置</h1>
<h2 id="deploy">Deploy</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.submit.deployMode</td>
          <td>none</td>
          <td>client或者cluster. client模式下提交的进程就是Driver, cluster模式下会新启一个机器作为Driver</td>
      </tr>
  </tbody>
</table>
<h2 id="driver">Driver</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.app.name</td>
          <td>none</td>
          <td>这个application的名称, 会显示在UI和log里, 重复也没关系</td>
      </tr>
      <tr>
          <td>spark.driver.cores</td>
          <td>1</td>
          <td>driver的核数, 只在cluster模式下生效</td>
      </tr>
      <tr>
          <td>spark.driver.memory</td>
          <td>1g</td>
          <td>driver的内存大小, 如果在client模式下, 需要在启动之前设置</td>
      </tr>
      <tr>
          <td>spark.driver.maxResultSize</td>
          <td>1g</td>
          <td>限制单个action下所有分区返回的序列化结果总大小, 至少为1M.  或者为0则不进行限制, 但是可能会造成Driver OOM异常</td>
      </tr>
  </tbody>
</table>
<h2 id="executor">Executor</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.executor.memory</td>
          <td>1g</td>
          <td>executor的内存大小</td>
      </tr>
      <tr>
          <td>spark.executor.memoryOverhead</td>
          <td>executorMemory * <code>spark.executor.memoryOverheadFactor</code>, with minimum of 384</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.executor.memoryOverheadFactor</td>
          <td>0.1</td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 id="runtime">Runtime</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.driver.extraClassPath</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.driver.defaultJavaOptions</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.driver.extraJavaOptions</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.driver.extraLibraryPath</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.driver.userClassPathFirst</td>
          <td>false</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.executor.extraClassPath</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.executor.defaultJavaOptions</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.executor.extraJavaOptions</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.executor.extraLibraryPath</td>
          <td>none</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.executor.userClassPathFirst</td>
          <td>false</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.files</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td>spark.jars</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 id="shuffle-behavior">Shuffle Behavior</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.reducer.maxSizeInFlight</td>
          <td>48m</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.reducer.maxReqsInFlight</td>
          <td>Int.MaxValue</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.reducer.maxBlocksInFlightPerAddress</td>
          <td>Int.MaxValue</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.compress</td>
          <td>true</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.file.buffer</td>
          <td>32k</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.io.maxRetries</td>
          <td>3</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.io.backLog</td>
          <td>-1</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.io.connectionTimeout</td>
          <td>value of <code>spark.network.timeout</code></td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.service.enabled</td>
          <td>false</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.service.removeShuffle</td>
          <td>false</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.sort.bypassMergeThreshold</td>
          <td>200</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.spill.compress</td>
          <td>true</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.files.io.connectionTimeout</td>
          <td>value of <code>spark.network.timeout</code></td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.checksum.enabled</td>
          <td>true</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.checksum.algorithm</td>
          <td>ADLER32</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.shuffle.service.fetch.rdd.enabled</td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 id="compression-and-serialization">Compression and Serialization</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.broadcast.compress</td>
          <td>true</td>
          <td>是否对广播变量做压缩,压缩格式使用<code>spark.io.compression.codec</code></td>
      </tr>
      <tr>
          <td>spark.rdd.compress</td>
          <td>false</td>
          <td>是否对序列化的RDD分区做压缩 ,压缩格式使用<code>spark.io.compression.codec</code></td>
      </tr>
      <tr>
          <td>spark.io.compression.codec</td>
          <td>lz4</td>
          <td>内部数据压缩格式, Spark默认提供了四种方式: lz4, lzf, snappy, zstd</td>
      </tr>
      <tr>
          <td>spark.io.compression.lz4.blockSize</td>
          <td>32k</td>
          <td>lz4压缩算法的区块大小</td>
      </tr>
      <tr>
          <td>spark.io.compression.snappy.blockSize</td>
          <td>32k</td>
          <td>snappy压缩算法的区块大小</td>
      </tr>
      <tr>
          <td>spark.io.compression.zstd.level</td>
          <td>1</td>
          <td>zstd压缩级别</td>
      </tr>
      <tr>
          <td>spark.io.compression.zstd.bufferSize</td>
          <td>32k</td>
          <td>zstd压缩算法的参数</td>
      </tr>
      <tr>
          <td>spark.kryoserializer.buffer.max</td>
          <td>64m</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.kryoserializer.buffer</td>
          <td>64k</td>
          <td></td>
      </tr>
      <tr>
          <td>spark.serializer</td>
          <td>org.apache.spark.serializer.JavaSerializer</td>
          <td>对象序列化方法, 默认值比较慢, 推荐使用<code>org.apache.spark.serializer.KryoSerializer</code> and configuring Kryo serialization</td>
      </tr>
  </tbody>
</table>
<h2 id="memory-management">Memory Management</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.memory.fraction</td>
          <td>0.6</td>
          <td>Execution + Storage Memory占用的比例</td>
      </tr>
      <tr>
          <td>spark.memory.storageFraction</td>
          <td>0.5</td>
          <td>Storage Memory的占用比例</td>
      </tr>
      <tr>
          <td>spark.memory.offHeap.enabled</td>
          <td>false</td>
          <td>是否使用堆外内存, 大小为<code>spark.memory.offHeap.size</code></td>
      </tr>
      <tr>
          <td>spark.memory.offHeap.size</td>
          <td>0</td>
          <td>堆外内存的大小</td>
      </tr>
  </tbody>
</table>
<h2 id="execution-behavior">Execution Behavior</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.broadcast.blockSize</td>
          <td>4m</td>
          <td>Size of each piece of a block for <code>TorrentBroadcastFactory</code></td>
      </tr>
      <tr>
          <td>spark.broadcast.checksum</td>
          <td>true</td>
          <td>使用需要使用checksum来进行数据校验</td>
      </tr>
      <tr>
          <td>spark.executor.cores</td>
          <td>1</td>
          <td>executor的核数</td>
      </tr>
      <tr>
          <td>spark.default.parallelism</td>
          <td></td>
          <td>默认的并行度</td>
      </tr>
      <tr>
          <td>spark.executor.heartbeatInterval</td>
          <td>10s</td>
          <td>每个executor与Driver的心跳间隔, 此参数要小于spark.network.timeout</td>
      </tr>
  </tbody>
</table>
<h2 id="network">Network</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.rpc.message.maxSize</td>
          <td>128</td>
          <td>RPC通信的消息大小, MB为单位</td>
      </tr>
      <tr>
          <td>spark.network.timeout</td>
          <td>120s</td>
          <td>超时时间</td>
      </tr>
      <tr>
          <td>spark.rpc.io.connectionTimeout</td>
          <td>value of <code>spark.network.timeout</code></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 id="scheduling">Scheduling</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.task.cpus</td>
          <td>1</td>
          <td>每个Task使用的核数</td>
      </tr>
  </tbody>
</table>
<h2 id="dynamic-allocation">Dynamic Allocation</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.dynamicAllocation.enabled</td>
          <td>false</td>
          <td>是否启用动态资源分配</td>
      </tr>
      <tr>
          <td>spark.dynamicAllocation.initialExecutors</td>
          <td>spark.dynamicAllocation.minExecutors</td>
          <td>Executor的初始数量</td>
      </tr>
      <tr>
          <td>spark.dynamicAllocation.maxExecutors</td>
          <td>infinity</td>
          <td>最大的executor数量</td>
      </tr>
      <tr>
          <td>spark.dynamicAllocation.minExecutors</td>
          <td>0</td>
          <td>最小的executor数量</td>
      </tr>
  </tbody>
</table>
<h2 id="spark-sql">Spark SQL</h2>
<table>
  <thead>
      <tr>
          <th>参数名称</th>
          <th>默认值</th>
          <th>参数说明</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>spark.sql.adaptive.enabled</td>
          <td>true</td>
          <td>是否开启自适应执行</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.coalescePartitions.enabled</td>
          <td>true</td>
          <td>是否开启自动分区合并</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.advisoryPartitionSizeInBytes</td>
          <td>value of <code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code></td>
          <td>shuffle时每个partition的数据量大小, 作用在合并小文件以及处理数据倾斜</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.coalescePartitions.minPartitionSize</td>
          <td>1MB</td>
          <td>shuffle partition的最小值</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.autoBroadcastJoinThreshold</td>
          <td></td>
          <td>广播表的阈值, 表大小小于这个值时会进行广播</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.skewJoin.enabled</td>
          <td>true</td>
          <td>开启后, 会对倾斜分区做拆分, 拆分后再进行join</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.skewJoin.skewedPartitionFactor</td>
          <td>5</td>
          <td>如何判断一个分区是否倾斜, 需要这个分区是中位数的N倍以上</td>
      </tr>
      <tr>
          <td>spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes</td>
          <td>256MB</td>
          <td>倾斜分区的大小还要大于这个值</td>
      </tr>
      <tr>
          <td>spark.sql.autoBroadcastJoinThreshold</td>
          <td>10MB</td>
          <td>小表自动广播的阈值</td>
      </tr>
      <tr>
          <td>spark.sql.broadcastTimeout</td>
          <td>300</td>
          <td>broadcast join时的超时时间(秒)</td>
      </tr>
  </tbody>
</table>
    </article>
  </main>
  

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css"
    integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js"
    integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp"
    crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js"
    integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"
    crossorigin="anonymous"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true }
            ],
            
            throwOnError: false
        });
    });
</script>





<script 
  src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"
  crossorigin="anonymous">
</script>
<script>
  mermaid.init(undefined, 'code.language-mermaid')
</script>




</div>


  <footer class="mt-8 mb-8">
  <div class="container mx-auto">
    <div class="mt-8 flex flex-col-reverse sm:flex-row sm:justify-between items-center">
      <div class="text-center sm:text-left">
        <p class="mt-0 text-sm"></p>
        <p class="mt-0 text-xs">
          Built with <a href="https://gohugo.io" target="_blank" rel="noopener noreferrer">Hugo</a> v0.143.1
          and <a href="https://github.com/mivinci/hugo-theme-minima" target="_blank" rel="noopener noreferrer">Minima</a>
        </p>
      </div>
      
      <p class="flex items-center mt-0">
        
          <a class="icon ml-1 mr-1" href="mailto:jarvis@apache.org" title="email">
          
            <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 21" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><path d="M22 6l-10 7L2 6"/></svg>
          
          </a>
        
          <a class="icon ml-1 mr-1" href="https://github.com/liunaijie" title="github">
          
            <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/></svg>
          
          </a>
        
          <a class="icon ml-1 mr-1" href="/index.xml" title="rss">
          
            <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 11a9 9 0 0 1 9 9M4 4a16 16 0 0 1 16 16"/><circle cx="5" cy="19" r="1"/></svg>
          
          </a>
        
      </p>
    </div>
  </div>
</footer>
</body>

</html>